# Hadoop-ApacheSpark-Analysis
This is the project done in collaboration with my colleagues Roberta Pappolla and Lorenzo Ferri. 
The scope of the project  was a simulation of a machine learning/data science project on a big dataset.
Thus,  a cluster computing framework was used: Hadoop/Apache Spark.
Various ML techniques were deployed: Classification, Clustering, Regression, DImensionality Reduction, Feature Engineering, etc.


Most notebooks have the comments added in Italian language, as this project was done at the University of Pisa.
